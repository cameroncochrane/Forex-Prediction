{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e0fa7b6",
   "metadata": {},
   "source": [
    "<h3>Forex Prediction: Recursive Neural Networks (RNNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3121b4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from data_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fd33ff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Date",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "EURO/US$",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "UNITED KINGDOM POUND/US$",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "YEN/US$",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "YUAN/US$",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "AUSTRALIAN DOLLAR/US$",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "5e2e962d-30c3-445d-a7d5-462abe81f409",
       "rows": [
        [
         "0",
         "2000-01-03 00:00:00",
         "0.9847",
         "0.6146",
         "101.7",
         "8.2798",
         "1.5172"
        ],
        [
         "1",
         "2000-01-04 00:00:00",
         "0.97",
         "0.6109",
         "103.09",
         "8.2799",
         "1.5239"
        ],
        [
         "2",
         "2000-01-05 00:00:00",
         "0.9676",
         "0.6092",
         "103.77",
         "8.2798",
         "1.5267"
        ],
        [
         "3",
         "2000-01-06 00:00:00",
         "0.9686",
         "0.607",
         "105.19",
         "8.2797",
         "1.5291"
        ],
        [
         "4",
         "2000-01-07 00:00:00",
         "0.9714",
         "0.6104",
         "105.17",
         "8.2794",
         "1.5272"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>EURO/US$</th>\n",
       "      <th>UNITED KINGDOM POUND/US$</th>\n",
       "      <th>YEN/US$</th>\n",
       "      <th>YUAN/US$</th>\n",
       "      <th>AUSTRALIAN DOLLAR/US$</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>0.9847</td>\n",
       "      <td>0.6146</td>\n",
       "      <td>101.70</td>\n",
       "      <td>8.2798</td>\n",
       "      <td>1.5172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-01-04</td>\n",
       "      <td>0.9700</td>\n",
       "      <td>0.6109</td>\n",
       "      <td>103.09</td>\n",
       "      <td>8.2799</td>\n",
       "      <td>1.5239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-01-05</td>\n",
       "      <td>0.9676</td>\n",
       "      <td>0.6092</td>\n",
       "      <td>103.77</td>\n",
       "      <td>8.2798</td>\n",
       "      <td>1.5267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-01-06</td>\n",
       "      <td>0.9686</td>\n",
       "      <td>0.6070</td>\n",
       "      <td>105.19</td>\n",
       "      <td>8.2797</td>\n",
       "      <td>1.5291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-01-07</td>\n",
       "      <td>0.9714</td>\n",
       "      <td>0.6104</td>\n",
       "      <td>105.17</td>\n",
       "      <td>8.2794</td>\n",
       "      <td>1.5272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  EURO/US$  UNITED KINGDOM POUND/US$  YEN/US$  YUAN/US$  \\\n",
       "0 2000-01-03    0.9847                    0.6146   101.70    8.2798   \n",
       "1 2000-01-04    0.9700                    0.6109   103.09    8.2799   \n",
       "2 2000-01-05    0.9676                    0.6092   103.77    8.2798   \n",
       "3 2000-01-06    0.9686                    0.6070   105.19    8.2797   \n",
       "4 2000-01-07    0.9714                    0.6104   105.17    8.2794   \n",
       "\n",
       "   AUSTRALIAN DOLLAR/US$  \n",
       "0                 1.5172  \n",
       "1                 1.5239  \n",
       "2                 1.5267  \n",
       "3                 1.5291  \n",
       "4                 1.5272  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data, cc_dict, countries =load_and_process_forex_data()\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d38c9208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_dict_currency(data,countries,currency_dict):\n",
    "\n",
    "    def extract_currency_data(data, currency):\n",
    "            \"\"\"\n",
    "            Extract a specific country's data from the main dataframe.\n",
    "            \n",
    "            Parameters:\n",
    "            data (DataFrame): The main dataframe containing all countries' data\n",
    "            country_name (str): The name of the country to extract data for\n",
    "            \n",
    "            Returns:\n",
    "            DataFrame: A dataframe containing only the specified country's data with date column\n",
    "            \"\"\"\n",
    "            country_data = data[['Date',currency]].copy()\n",
    "            \n",
    "            # Ensure date column is included and properly formatted\n",
    "            \n",
    "            return country_data\n",
    "\n",
    "    data_dict = {}\n",
    "    for country in countries:\n",
    "        data_dict[currency_dict[country]] = extract_currency_data(data,currency_dict[country])\n",
    "    \n",
    "    return data_dict    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7cfc6bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Date",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "EURO/US$",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "629d1d2e-e921-41a2-ac2d-f5c9be39d2b8",
       "rows": [
        [
         "0",
         "2000-01-03 00:00:00",
         "0.9847"
        ],
        [
         "1",
         "2000-01-04 00:00:00",
         "0.97"
        ],
        [
         "2",
         "2000-01-05 00:00:00",
         "0.9676"
        ],
        [
         "3",
         "2000-01-06 00:00:00",
         "0.9686"
        ],
        [
         "4",
         "2000-01-07 00:00:00",
         "0.9714"
        ],
        [
         "5",
         "2000-01-10 00:00:00",
         "0.9754"
        ],
        [
         "6",
         "2000-01-11 00:00:00",
         "0.9688"
        ],
        [
         "7",
         "2000-01-12 00:00:00",
         "0.9727"
        ],
        [
         "8",
         "2000-01-13 00:00:00",
         "0.9737"
        ],
        [
         "9",
         "2000-01-14 00:00:00",
         "0.9874"
        ],
        [
         "11",
         "2000-01-18 00:00:00",
         "0.988"
        ],
        [
         "12",
         "2000-01-19 00:00:00",
         "0.9886"
        ],
        [
         "13",
         "2000-01-20 00:00:00",
         "0.9869"
        ],
        [
         "14",
         "2000-01-21 00:00:00",
         "0.9901"
        ],
        [
         "15",
         "2000-01-24 00:00:00",
         "0.9981"
        ],
        [
         "16",
         "2000-01-25 00:00:00",
         "0.9959"
        ],
        [
         "17",
         "2000-01-26 00:00:00",
         "0.9989"
        ],
        [
         "18",
         "2000-01-27 00:00:00",
         "1.0111"
        ],
        [
         "19",
         "2000-01-28 00:00:00",
         "1.0241"
        ],
        [
         "20",
         "2000-01-31 00:00:00",
         "1.0249"
        ],
        [
         "21",
         "2000-02-01 00:00:00",
         "1.0276"
        ],
        [
         "22",
         "2000-02-02 00:00:00",
         "1.0238"
        ],
        [
         "23",
         "2000-02-03 00:00:00",
         "1.0114"
        ],
        [
         "24",
         "2000-02-04 00:00:00",
         "1.0246"
        ],
        [
         "25",
         "2000-02-07 00:00:00",
         "1.0222"
        ],
        [
         "26",
         "2000-02-08 00:00:00",
         "1.014"
        ],
        [
         "27",
         "2000-02-09 00:00:00",
         "1.0087"
        ],
        [
         "28",
         "2000-02-10 00:00:00",
         "1.0137"
        ],
        [
         "29",
         "2000-02-11 00:00:00",
         "1.0155"
        ],
        [
         "30",
         "2000-02-14 00:00:00",
         "1.0222"
        ],
        [
         "31",
         "2000-02-15 00:00:00",
         "1.0169"
        ],
        [
         "32",
         "2000-02-16 00:00:00",
         "1.0161"
        ],
        [
         "33",
         "2000-02-17 00:00:00",
         "1.0139"
        ],
        [
         "34",
         "2000-02-18 00:00:00",
         "1.0152"
        ],
        [
         "36",
         "2000-02-22 00:00:00",
         "0.994"
        ],
        [
         "37",
         "2000-02-23 00:00:00",
         "0.9983"
        ],
        [
         "38",
         "2000-02-24 00:00:00",
         "1.0069"
        ],
        [
         "39",
         "2000-02-25 00:00:00",
         "1.0243"
        ],
        [
         "40",
         "2000-02-28 00:00:00",
         "1.0342"
        ],
        [
         "41",
         "2000-02-29 00:00:00",
         "1.037"
        ],
        [
         "42",
         "2000-03-01 00:00:00",
         "1.0309"
        ],
        [
         "43",
         "2000-03-02 00:00:00",
         "1.0396"
        ],
        [
         "44",
         "2000-03-03 00:00:00",
         "1.0397"
        ],
        [
         "45",
         "2000-03-06 00:00:00",
         "1.0413"
        ],
        [
         "46",
         "2000-03-07 00:00:00",
         "1.046"
        ],
        [
         "47",
         "2000-03-08 00:00:00",
         "1.0443"
        ],
        [
         "48",
         "2000-03-09 00:00:00",
         "1.0326"
        ],
        [
         "49",
         "2000-03-10 00:00:00",
         "1.0353"
        ],
        [
         "50",
         "2000-03-13 00:00:00",
         "1.0365"
        ],
        [
         "51",
         "2000-03-14 00:00:00",
         "1.0369"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5019
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>EURO/US$</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>0.9847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-01-04</td>\n",
       "      <td>0.9700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-01-05</td>\n",
       "      <td>0.9676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-01-06</td>\n",
       "      <td>0.9686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-01-07</td>\n",
       "      <td>0.9714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5211</th>\n",
       "      <td>2019-12-24</td>\n",
       "      <td>0.9022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5213</th>\n",
       "      <td>2019-12-26</td>\n",
       "      <td>0.9007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5214</th>\n",
       "      <td>2019-12-27</td>\n",
       "      <td>0.8949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5215</th>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>0.8915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5216</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>0.8907</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5019 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date  EURO/US$\n",
       "0    2000-01-03    0.9847\n",
       "1    2000-01-04    0.9700\n",
       "2    2000-01-05    0.9676\n",
       "3    2000-01-06    0.9686\n",
       "4    2000-01-07    0.9714\n",
       "...         ...       ...\n",
       "5211 2019-12-24    0.9022\n",
       "5213 2019-12-26    0.9007\n",
       "5214 2019-12-27    0.8949\n",
       "5215 2019-12-30    0.8915\n",
       "5216 2019-12-31    0.8907\n",
       "\n",
       "[5019 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dictionary of dataframes of each currency\n",
    "data_dict = create_data_dict_currency(all_data,countries,cc_dict)\n",
    "data_dict['EURO/US$']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a5e78a",
   "metadata": {},
   "source": [
    "<h5>Pre-processing to carry out:</h5>\n",
    "\n",
    "\n",
    "- Train-test split.\n",
    "- Scaling (will use a RobustScaler()). Fit_transform on train, and transform on test.\n",
    "- Time window creation.\n",
    "\n",
    "\n",
    "N.B. Save the scalers used for each currency to be used for data preparation and inversion when using the model in the app (as pickle files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44305342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test splitting (non-random, last 60 rows are taken as the test). Will focus on EUR now:\n",
    "df = data_dict['EURO/US$'].copy()\n",
    "if 'Date' in df.columns:\n",
    "    df = df.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "test_df = df.tail(60).reset_index(drop=True)\n",
    "train_df = df.iloc[:-60].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2687523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "def robust_scale_train_test(train_df, test_df, drop_date=True):\n",
    "    \"\"\"\n",
    "    Fit RobustScaler on train_df (drop Date column if present) and transform both train and test.\n",
    "    Returns: scaler, train_scaled_df, test_scaled_df\n",
    "    \"\"\"\n",
    "    if drop_date and 'Date' in train_df.columns:\n",
    "        X_train = train_df.drop(columns=['Date']).copy()\n",
    "    else:\n",
    "        X_train = train_df.copy()\n",
    "    if drop_date and 'Date' in test_df.columns:\n",
    "        X_test = test_df.drop(columns=['Date']).copy()\n",
    "    else:\n",
    "        X_test = test_df.copy()\n",
    "\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train.values)\n",
    "    X_test_scaled = scaler.transform(X_test.values)\n",
    "\n",
    "    train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "    test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "    return scaler, train_scaled_df, test_scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0ade38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eur_scaler, train_scaled_df, test_scaled_df = robust_scale_train_test(train_df,test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdb5c1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create windows for an LSTM model:\n",
    "def LSTM_input(df, input_sequence):\n",
    "    \"\"\"\n",
    "    Generate supervised learning sequences from a time-ordered dataset\n",
    "    for one-step-ahead LSTM forecasting.\n",
    "\n",
    "    This function converts a time-series DataFrame into sliding input\n",
    "    sequences (X) and corresponding target values (y), suitable for\n",
    "    training a many-to-one LSTM model.\n",
    "\n",
    "    For each sample:\n",
    "        - X contains `input_sequence` consecutive past observations\n",
    "        - y contains the immediately following observation\n",
    "\n",
    "    The function assumes the data is:\n",
    "        - Ordered in ascending chronological order (oldest → newest)\n",
    "        - Free of non-numeric columns (e.g. time columns removed)\n",
    "        - Already scaled or normalised, if required\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Time-series data containing one or more numeric features.\n",
    "        Shape: (n_samples, n_features).\n",
    "        The index or original time column is not used by this function.\n",
    "\n",
    "    input_sequence : int\n",
    "        Number of past time steps to include in each input sequence\n",
    "        (i.e. the LSTM lookback window).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : numpy.ndarray\n",
    "        Array of input sequences with shape:\n",
    "            (n_samples - input_sequence, input_sequence, n_features)\n",
    "\n",
    "    y : numpy.ndarray\n",
    "        Array of target values with shape:\n",
    "            (n_samples - input_sequence, n_features)\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - This function performs one-step-ahead forecasting.\n",
    "    - Each target value corresponds to the time step immediately\n",
    "      following its input sequence.\n",
    "    - The function does not shuffle data and preserves temporal order.\n",
    "    - The function does not perform any scaling or missing-value handling.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> X, y = Sequential_Input_LSTM(df_scaled, input_sequence=28)\n",
    "    >>> X.shape\n",
    "    (num_samples, 28, num_features)\n",
    "    >>> y.shape\n",
    "    (num_samples, num_features)\n",
    "    \"\"\"\n",
    "    df_np = df.to_numpy()\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for i in range(len(df_np) - input_sequence):\n",
    "        row = [a for a in df_np[i:i + input_sequence]]\n",
    "        X.append(row)\n",
    "        label = df_np[i + input_sequence]\n",
    "        y.append(label)\n",
    "        \n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38260fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the windowed train and test (scaled) data:\n",
    "train_windowed_X, train_windowed_y = LSTM_input(train_scaled_df,10)\n",
    "test_windowed_X, test_windowed_y = LSTM_input(test_scaled_df, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf7bc408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data is now split and preprocessed ready for modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf94cb8",
   "metadata": {},
   "source": [
    "<h5>Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987a0b3e",
   "metadata": {},
   "source": [
    "- Will create an optimise a model for the EUR/USD set and then train the same type of model on the other currencies.\n",
    "\n",
    "- Will use a temporal split for the validation instead of just creating a validation set (from the training set, manually) and providing it i.e. I will define a fraction (of the last rows) that will be taken by the model for validation from the training set. Validation isn't a necessity but it is useful for model evaluation.\n",
    "\n",
    "- Will start with a single LSTM layer and then add more layers if evaluation shows that it is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74ff9444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 2.20.0\n",
      "GPUs: []\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Check if the GPU is being used:\n",
    "print(\"TF version:\", tf.__version__)\n",
    "print(\"GPUs:\", tf.config.list_physical_devices(\"GPU\"))\n",
    "# Import others:\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9877a529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# May need to do training on the PC (not mac) as the training speed is poor on mac. Cunrrently running\n",
    "# python 3.12 which isn't compatiable with tensorflow-metal (allows GPU training on the Mac M3 chip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefe9780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Starting with a single LSTM layer model\n",
    "X_train = train_windowed_X\n",
    "y_train = train_windowed_y\n",
    "\n",
    "lookback_length = X_train.shape[1] # Should be 10 here.\n",
    "features = X_train.shape[2] # Should be 1 here.\n",
    "\n",
    "print(lookback_length)\n",
    "print(features)\n",
    "\n",
    "Train_1 = False\n",
    "while Train_1 == True:\n",
    "\n",
    "    eur_model_1 = keras.Sequential([\n",
    "        keras.layers.Input(shape=(lookback_length,features)),\n",
    "        keras.layers.LSTM(32, return_sequences=False), # If not False, the dimnesionality of the predictions can be = rows*look_back_length\n",
    "        keras.layers.Dropout(0.3),\n",
    "        keras.layers.Dense(1, activation = 'linear') #This is a regression problem of continuous and unbound values. No artificial constraints should be placed.\n",
    "    ])\n",
    "\n",
    "    eur_model_1.summary()\n",
    "\n",
    "    # Train the model:\n",
    "    # Model training here is slow. Make sure to optimise the model structure and compilation/training parameters.\n",
    "    # Early stopping to reduce overfitting:\n",
    "\n",
    "    early_stop = EarlyStopping(monitor = 'val_loss', patience = 5)\n",
    "\n",
    "    eur_model_1.compile(loss = MeanSquaredError(),\n",
    "                        optimizer = Adam(learning_rate = 0.01), \n",
    "                        metrics = [RootMeanSquaredError()])\n",
    "\n",
    "    eur_model_1_history = eur_model_1.fit(\n",
    "        X_train, y_train,\n",
    "        validation_split=0.1, # Last 10% of the training set is taken as the validation set: a 'temporal' split.\n",
    "        epochs=50,\n",
    "        batch_size=256,\n",
    "        shuffle=False,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=1\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6907d7a",
   "metadata": {},
   "source": [
    "Forecasting Functionality:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d415da",
   "metadata": {},
   "source": [
    "Things needed for forecasting in the streamlit app:\n",
    "- A way to load the model from a pkl file.\n",
    "- For 'execute_rnn()', a way to use the model, processed data (i.e. windowed) for a single currency, and a defined forecast length.\n",
    "- The processed data (windowed) should be generated when the app.py is first run. A function to generate windowed data for all currencies should be run in the 'if __name__ == etc.. ' block. Call it something similar to this:\n",
    "\n",
    "    xgbdata_raw = create_data_dict_currency_xgboost(data,country_names,country_currency_dict)\n",
    "    xgbdata_processed = process_all_xgboost(xgbdata_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da76f09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def incremental_forecast_to_df(\n",
    "    model,\n",
    "    history,\n",
    "    lookback,\n",
    "    horizon=60,\n",
    "    scaler=None,\n",
    "    start_step=1,\n",
    "    column_name=\"forecast\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Incremental (recursive) forecasting for a univariate windowed TF RNN model.\n",
    "\n",
    "    Assumptions:\n",
    "      - Univariate series (single column)\n",
    "      - Model input shape: (batch, lookback, 1)\n",
    "      - Model output: next value as (batch, 1) or (batch,)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model:\n",
    "        A pre-trained tf.keras model (or compatible object with .predict()).\n",
    "    history:\n",
    "        Most recent observed values. If scaler is provided, these should be UNscaled.\n",
    "        Must contain at least `lookback` points.\n",
    "    lookback:\n",
    "        Window length used during training.\n",
    "    horizon:\n",
    "        Forecast length, e.g. 60 \"days\" (steps).\n",
    "    scaler:\n",
    "        Optional fitted scaler (e.g. sklearn StandardScaler) fit on training data.\n",
    "        If provided, history is scaled internally; outputs are inverse-transformed.\n",
    "    start_step:\n",
    "        Starting integer index for the forecast steps (default 1).\n",
    "    column_name:\n",
    "        Name of the forecast column in the output DataFrame.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame with integer index [start_step ... start_step + horizon - 1]\n",
    "    \"\"\"\n",
    "    # --- validate & coerce history ---\n",
    "    hist = np.asarray(history, dtype=np.float32).reshape(-1, 1)\n",
    "    if hist.shape[0] < lookback:\n",
    "        raise ValueError(\n",
    "            f\"`history` must have at least {lookback} observations; got {hist.shape[0]}.\"\n",
    "        )\n",
    "    if horizon < 1:\n",
    "        raise ValueError(\"`horizon` must be >= 1.\")\n",
    "    if lookback < 1:\n",
    "        raise ValueError(\"`lookback` must be >= 1.\")\n",
    "\n",
    "    # --- scale if needed ---\n",
    "    if scaler is not None:\n",
    "        hist_scaled = scaler.transform(hist)  # shape (n, 1)\n",
    "    else:\n",
    "        hist_scaled = hist  # already in training scale\n",
    "\n",
    "    # --- rolling window (scaled) ---\n",
    "    window = hist_scaled[-lookback:].astype(np.float32).reshape(1, lookback, 1)\n",
    "\n",
    "    preds_scaled = np.zeros((horizon, 1), dtype=np.float32)\n",
    "\n",
    "    for t in range(horizon):\n",
    "        yhat = model.predict(window, verbose=0)\n",
    "        yhat = np.asarray(yhat).reshape(-1)  # handles (1,1) or (1,)\n",
    "        yhat_val = np.float32(yhat[0])\n",
    "\n",
    "        preds_scaled[t, 0] = yhat_val\n",
    "\n",
    "        # update window: drop oldest timestep, append prediction\n",
    "        window = np.concatenate([window[:, 1:, :], [[[yhat_val]]]], axis=1)\n",
    "\n",
    "    # --- inverse transform if scaler provided ---\n",
    "    if scaler is not None:\n",
    "        preds = scaler.inverse_transform(preds_scaled)\n",
    "    else:\n",
    "        preds = preds_scaled\n",
    "\n",
    "    # --- build DataFrame with integer step index ---\n",
    "    steps = np.arange(start_step, start_step + horizon, dtype=int)\n",
    "    out = pd.DataFrame({column_name: preds.reshape(-1)}, index=steps)\n",
    "    out.index.name = \"step\"\n",
    "    return out\n",
    "\n",
    "\n",
    "def forecast_from_pickled_model_to_df(\n",
    "    model,\n",
    "    history,\n",
    "    lookback,\n",
    "    horizon=60,\n",
    "    scaler=None,\n",
    "    start_step=1,\n",
    "    column_name=\"forecast\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Convenience wrapper:\n",
    "      - Provide a pre-trained model\n",
    "      - Runs incremental forecast\n",
    "      - Returns results as a DataFrame with integer step index\n",
    "    \"\"\"\n",
    "    return incremental_forecast_to_df(\n",
    "        model=model,\n",
    "        history=history,\n",
    "        lookback=lookback,\n",
    "        horizon=horizon,\n",
    "        scaler=scaler,\n",
    "        start_step=start_step,\n",
    "        column_name=column_name,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
